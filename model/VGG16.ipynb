{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "process.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9ufacpzL8WI",
        "outputId": "412fcc88-d329-402f-8b73-f41e9da476ca"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXWYS6nUn12c"
      },
      "source": [
        "# 깃헙 동기화 및 경로 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCRaI8IpkAdg",
        "outputId": "f4ca0ed3-3225-4d15-c9d0-052ab03a582c"
      },
      "source": [
        "%cd /content/drive/MyDrive/Final_project\n",
        "!git clone https://github.com/Slangoij/PlayData_Final_Project.git\n",
        "%cd /content/drive/MyDrive/Final_project/PlayData_Final_Project"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Final_project\n",
            "fatal: destination path 'PlayData_Final_Project' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/Final_project/PlayData_Final_Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmBAZOKsnoau"
      },
      "source": [
        "# 파라미터 및 데이터 경로 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txdf-64DVr2p"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import VGG16, ResNet50V2, MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# 하이퍼 파라미터\n",
        "LEARNING_RATE = 0.001\n",
        "N_EPOCHS = 20\n",
        "N_BATCHS = 20\n",
        "NUM_CLASSES = 3 # 클래스 개수\n",
        "CLASS_MODE = 'categorical'\n",
        "INPUT_SHAPE = (640, 480, 3) # 학습할 이미지 shape\n",
        "FEATURE_SHAPE = (20, 15, 512) # 모델에 맞춰 변경해야 한다!\n",
        "\n",
        "# 학습데이터 및 모델 경로 설정\n",
        "train_dir = './inputdata_preprocessing/classedImg/train'\n",
        "validation_dir = './inputdata_preprocessing/classedImg/val'\n",
        "test_dir = './inputdata_preprocessing/classedImg/test'\n",
        "MODEL_PATH = './model/vgg16'"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-R0TYN_AqMK"
      },
      "source": [
        "# 함수: 데이터 증강, 특성추출, 분류기 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p1JfCCXAo3z"
      },
      "source": [
        "# 데이터 증강\n",
        "def get_generators():\n",
        "    '''\n",
        "    train, validation, test generator를 생성해서 반환.\n",
        "    train generator는 image 변환 처리\n",
        "    '''\n",
        "    # 상하좌우 이동, 확대, 회전 이미지로 증강\n",
        "    train_datagen = ImageDataGenerator(rescale=1/255,\n",
        "                                       rotation_range=20,\n",
        "                                       zoom_range=0.5,\n",
        "                                       height_shift_range=0.3,\n",
        "                                       width_shift_range=0.3)\n",
        "    \n",
        "    test_datagen = ImageDataGenerator(rescale=1/255) #validation/test에서 사용\n",
        "\n",
        "    # generator 들 생성\n",
        "    # 첫번째는 먼저 한동작이므로 동작과 비동작으로 바이너리 구분만\n",
        "    train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                        target_size=INPUT_SHAPE[:2],\n",
        "                                                        batch_size=N_BATCHS,\n",
        "                                                        class_mode=CLASS_MODE)    \n",
        "    val_generator = test_datagen.flow_from_directory(validation_dir,\n",
        "                                                     target_size=INPUT_SHAPE[:2],\n",
        "                                                     batch_size=N_BATCHS,\n",
        "                                                     class_mode=CLASS_MODE)\n",
        "    test_generator = test_datagen.flow_from_directory(test_dir,\n",
        "                                                      target_size=INPUT_SHAPE[:2],\n",
        "                                                      batch_size=N_BATCHS,\n",
        "                                                      class_mode=CLASS_MODE)\n",
        "    return train_generator, val_generator, test_generator\n",
        "\n",
        "\n",
        "# 특성 추출로 빠른 학습\n",
        "def extract_featuremap(image_directory, sample_counts):\n",
        "  \"\"\"\n",
        "  매개변수로 받은 디렉토리의 이미지를 Conv_base(VGG16) 모델을 통과시켜 Featuremap을 추출해 반환하는 함수\n",
        "  [매개변수]\n",
        "    image_directory: 이미지 데이터들이 있는 디렉토리\n",
        "    sample_counts: 특성을 추출할 이미지 개수\n",
        "  [반환값]\n",
        "    튜플: (featuremap들, label)\n",
        "  \"\"\"\n",
        "  conv_base = VGG16(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
        "\n",
        "  # 결과를 담을 ndarray\n",
        "  return_features = np.zeros(shape=(sample_counts, 20, 15, 512)) # Featuremap저장, conv_base의 마지막 layer의 output의 shape에 맞춘다.\n",
        "  return_labels = np.zeros(shape=(sample_counts,NUM_CLASSES)) # label 저장\n",
        "\n",
        "  datagen = ImageDataGenerator(rescale=1./255)\n",
        "  iterator = datagen.flow_from_directory(image_directory,\n",
        "                                         target_size=INPUT_SHAPE[:2],\n",
        "                                         batch_size=N_BATCHS,\n",
        "                                         class_mode=CLASS_MODE)\n",
        "  i = 0 # 반복횟수 저장할 변수\n",
        "  for input_batch, label_batch in iterator: # (image, label) * batch크기(100)\n",
        "    # input_batch를 conv_base 넣어서 featuremap을 추출\n",
        "    fm = conv_base.predict(input_batch)\n",
        "\n",
        "    return_features[i*N_BATCHS: (i+1)*N_BATCHS] = fm\n",
        "    return_labels[i*N_BATCHS: (i+1)*N_BATCHS] = label_batch\n",
        "\n",
        "    i+=1\n",
        "    if i*N_BATCHS >= sample_counts: # 결과를 저장할 배열의 시작index가 sample_counts보다 크면 반복문 멈추기\n",
        "      break\n",
        "\n",
        "  return return_features, return_labels\n",
        "\n",
        "\n",
        "def create_model():\n",
        "  # 분류기 모델만 생성\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.Input(FEATURE_SHAPE))\n",
        "  model.add(layers.GlobalAveragePooling2D())\n",
        "  model.add(layers.Dropout(rate=0.5))\n",
        "  model.add(layers.Dense(256, activation='relu'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dense(NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "  return model\n",
        "\n",
        "# 결과 출력\n",
        "def plot_result(history, ymin=None, ymax=None):\n",
        "    plt.figure(figsize=(15,5))\n",
        "    plt.subplot(1,2,1)\n",
        "\n",
        "    plt.plot(range(1,N_EPOCHS+1), history.history['loss'], label='train loss')\n",
        "    plt.plot(range(1,N_EPOCHS+1), history.history['val_loss'], label='validation loss')\n",
        "    plt.title('LOSS')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    if ymin!=None and ymax!=None:\n",
        "        plt.ylim(ymin, ymax)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(range(1, N_EPOCHS+1), history.history['accuracy'], label='train accuracy')\n",
        "    plt.plot(range(1, N_EPOCHS+1), history.history['val_accuracy'], label='validation accuracy')\n",
        "    plt.title('ACCURACY')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    if ymin!=None and ymax!=None:\n",
        "        plt.ylim(ymin, ymax)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7iP2TiQntAS"
      },
      "source": [
        "# 학습 이미지 정리(디렉토리, 이름, 파일 개수)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQQQL_SjuEfg",
        "outputId": "26ac360a-fb6e-4b40-fc3f-39cb1e105f79"
      },
      "source": [
        "%cd /content/drive/MyDrive/Final_project/PlayData_Final_Project"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Final_project/PlayData_Final_Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8Wpkt643KYG"
      },
      "source": [
        "import shutil\n",
        "# 기존 데이터 디렉토리 지우기\n",
        "# shutil.rmtree(train_dir, ignore_errors=True)\n",
        "# shutil.rmtree(validation_dir, ignore_errors=True)\n",
        "# shutil.rmtree(test_dir, ignore_errors=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja36ZkHbnUc3"
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(validation_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "ORG_IMG_PATH = \"./inputdata_preprocessing/img\"\n",
        "img_file_list = os.listdir(ORG_IMG_PATH)\n",
        "\n",
        "for i in range(len(img_file_list)):\n",
        "  train_under_dir = train_dir + '/' + str(i) + '/'\n",
        "  validation_under_dir = validation_dir + '/' + str(i) + '/'\n",
        "  test_under_dir = test_dir + '/' + str(i) + '/'\n",
        "  \n",
        "  os.makedirs(train_under_dir, exist_ok=True)\n",
        "  os.makedirs(validation_under_dir, exist_ok=True)\n",
        "  os.makedirs(test_under_dir, exist_ok=True)\n",
        "\n",
        "  tmp_img_path = os.path.join(ORG_IMG_PATH, img_file_list[i])\n",
        "  img_list = os.listdir(tmp_img_path)\n",
        "  train_len = int(len(img_list)*0.7) # 70%를 train으로 넣어줌\n",
        "  val_len = int(len(img_list)*0.9) # 나머지 20%를 train으로 넣어줌 나머지는 test\n",
        "  random.shuffle(img_list)\n",
        "\n",
        "  count = 0\n",
        "  for img_name in img_list:\n",
        "      if count < train_len:\n",
        "          shutil.copy(tmp_img_path + '/' + img_name, train_dir + '/' + str(i) + '/' + img_name) # 이미지 copy\n",
        "      elif count < val_len:\n",
        "        shutil.copy(tmp_img_path + '/' + img_name, validation_dir + '/' + str(i) + '/' + img_name)\n",
        "      else:\n",
        "        shutil.copy(tmp_img_path + '/' + img_name, test_dir + '/' + str(i) + '/' + img_name)\n",
        "      count += 1\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U69jWWa9xc7Z"
      },
      "source": [
        "# 메인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPKafL6T6ePA",
        "outputId": "a5649988-0788-463c-d06b-e78ac8966047"
      },
      "source": [
        "data_cnts"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[41, 12, 7]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7N5I4sCWIhw",
        "outputId": "7fbf1e4f-4565-4f36-ed60-dbe6ddd9fb02"
      },
      "source": [
        "# train, val, test 각 폴더 내 데이터 개수\n",
        "data_cnts = []\n",
        "for dirs in [train_dir, validation_dir, test_dir]:\n",
        "  cnt = 0\n",
        "  for (path, dir, files) in os.walk(dirs):\n",
        "    cnt += len(files)\n",
        "  data_cnts.append(cnt)\n",
        "\n",
        "# Featuremap 추출\n",
        "train_features, train_labels = extract_featuremap(train_dir, data_cnts[0])\n",
        "validation_features, validation_labels = extract_featuremap(validation_dir, data_cnts[1])\n",
        "test_features, test_labels = extract_featuremap(test_dir, data_cnts[2])\n",
        "\n",
        "mc_callback = keras.callbacks.ModelCheckpoint(MODEL_PATH, monitor='val_loss', save_best_only=True)\n",
        "\n",
        "train_iterator, validation_iterator, test_iterator = get_generators()\n",
        "\n",
        "model = create_model()\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE), \n",
        "              loss='categorical_crossentropy', # 클래스 따라 변경\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# 특성 추출 없이\n",
        "# history = model.fit(train_iterator, epochs=N_EPOCHS,\n",
        "#                     steps_per_epoch=len(train_iterator),\n",
        "#                     validation_data=validation_iterator,\n",
        "#                     validation_steps=len(validation_iterator),\n",
        "#                     callbacks=[mc_callback])\n",
        "\n",
        "# 특성 추출해서\n",
        "history = model.fit(train_features, train_labels,\n",
        "                    epochs=N_EPOCHS,\n",
        "                    validation_data=(validation_features, validation_labels),\n",
        "                    batch_size=N_BATCHS,\n",
        "                    callbacks=[mc_callback])\n",
        "\n",
        "best_model = keras.models.load_model(MODEL_PATH)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 51 images belonging to 3 classes.\n",
            "Found 22 images belonging to 3 classes.\n",
            "Found 12 images belonging to 3 classes.\n",
            "WARNING:tensorflow:5 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7faf4f38a560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Found 51 images belonging to 3 classes.\n",
            "Found 22 images belonging to 3 classes.\n",
            "Found 12 images belonging to 3 classes.\n",
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 133,123\n",
            "Trainable params: 132,611\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "3/3 [==============================] - 1s 138ms/step - loss: 1.3484 - accuracy: 0.4314 - val_loss: 1.1017 - val_accuracy: 0.3636\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n",
            "Epoch 2/20\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1755 - accuracy: 0.3529 - val_loss: 1.0892 - val_accuracy: 0.3636\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n",
            "Epoch 3/20\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1726 - accuracy: 0.4118 - val_loss: 1.0811 - val_accuracy: 0.4545\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n",
            "Epoch 4/20\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 1.1875 - accuracy: 0.3725 - val_loss: 1.0856 - val_accuracy: 0.3182\n",
            "Epoch 5/20\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 1.0678 - accuracy: 0.5098 - val_loss: 1.0910 - val_accuracy: 0.3182\n",
            "Epoch 6/20\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2017 - accuracy: 0.4510 - val_loss: 1.0851 - val_accuracy: 0.3182\n",
            "Epoch 7/20\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1146 - accuracy: 0.5098 - val_loss: 1.0692 - val_accuracy: 0.3636\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n",
            "Epoch 8/20\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.2279 - accuracy: 0.3529 - val_loss: 1.0485 - val_accuracy: 0.5455\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n",
            "Epoch 9/20\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 1.0194 - accuracy: 0.4706 - val_loss: 1.0242 - val_accuracy: 0.5000\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n",
            "Epoch 10/20\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.1378 - accuracy: 0.4510 - val_loss: 1.0048 - val_accuracy: 0.4545\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n",
            "Epoch 11/20\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 1.1300 - accuracy: 0.3529 - val_loss: 0.9946 - val_accuracy: 0.3636\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n",
            "Epoch 12/20\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.9260 - accuracy: 0.5686 - val_loss: 0.9805 - val_accuracy: 0.3636\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n",
            "Epoch 13/20\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 1.0733 - accuracy: 0.4510 - val_loss: 0.9655 - val_accuracy: 0.5909\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n",
            "Epoch 14/20\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8856 - accuracy: 0.5686 - val_loss: 0.9543 - val_accuracy: 0.7273\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n",
            "Epoch 15/20\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8649 - accuracy: 0.5882 - val_loss: 0.9438 - val_accuracy: 0.8182\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n",
            "Epoch 16/20\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.8626 - accuracy: 0.5490 - val_loss: 0.9344 - val_accuracy: 0.8636\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n",
            "Epoch 17/20\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.9243 - accuracy: 0.4902 - val_loss: 0.9256 - val_accuracy: 0.9091\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n",
            "Epoch 18/20\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.9389 - accuracy: 0.6078 - val_loss: 0.9170 - val_accuracy: 0.8636\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n",
            "Epoch 19/20\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8388 - accuracy: 0.6078 - val_loss: 0.9056 - val_accuracy: 0.5909\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n",
            "Epoch 20/20\n",
            "3/3 [==============================] - 0s 32ms/step - loss: 0.7515 - accuracy: 0.6471 - val_loss: 0.8960 - val_accuracy: 0.5000\n",
            "INFO:tensorflow:Assets written to: ./model/vgg16/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awIe2pJ29W47",
        "outputId": "3ff65a45-1b35-4cd9-e928-bef2043799f1"
      },
      "source": [
        "# 모델 합치기\n",
        "whole_model = keras.Sequential()\n",
        "whole_model.add(VGG16(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE))\n",
        "whole_model.add(best_model)\n",
        "whole_model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE), \n",
        "              loss='binary_crossentropy', # 클래스 따라 변경\n",
        "              metrics=['accuracy'])\n",
        "whole_model.summary()\n",
        "\n",
        "# evaluation\n",
        "whole_model.evaluate(train_iterator)\n",
        "whole_model.evaluate(test_iterator)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Functional)           (None, 20, 15, 512)       14714688  \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      (None, 3)                 133123    \n",
            "=================================================================\n",
            "Total params: 14,847,811\n",
            "Trainable params: 14,847,299\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n",
            "3/3 [==============================] - 6s 1s/step - loss: 0.6335 - accuracy: 0.4118\n",
            "1/1 [==============================] - 1s 907ms/step - loss: 0.6183 - accuracy: 0.6667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6182629466056824, 0.6666666865348816]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdsZbVGBiSJS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxo76DY4AHTA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}